### Aphasia Demo System - Streamlit Demo App

import streamlit as st
import os
from main import SpeechRecognizer, LLMProcessor, TextToSpeech, Audio2FaceConnector, SpeechAssessmentModule, USER_WAV, OUTPUT_WAV, A2F_INSTANCE

# åˆå§‹åŒ–æ¨¡å—
speech_recognizer = SpeechRecognizer()
llm_processor = LLMProcessor()
text_to_speech = TextToSpeech()
a2f_connector = Audio2FaceConnector()
speech_assessment = SpeechAssessmentModule()

# é¡µé¢è®¾ç½®
st.set_page_config(page_title="å¤±è¯­ç—‡åº·å¤è®­ç»ƒ Demo", layout="centered")
st.title("ğŸ§  åŸºäºå¤šæ¨¡æ€æ™ºèƒ½ä½“çš„å¤±è¯­ç—‡è¨€è¯­è®­ç»ƒç³»ç»Ÿ")

st.sidebar.title("ğŸ“‹ åŠŸèƒ½é€‰æ‹©")
mode = st.sidebar.radio("è¯·é€‰æ‹©åŠŸèƒ½æ¨¡å¼ï¼š", ["æ­£å¸¸å¯¹è¯", "è¯­éŸ³è¯„æµ‹"])

if mode == "æ­£å¸¸å¯¹è¯":
    st.header("ğŸ™ï¸ è¯­éŸ³å¯¹è¯")
    if st.button("ç‚¹å‡»å¼€å§‹å¯¹è¯"):
        with st.spinner("æ­£åœ¨è¯†åˆ«æ‚¨çš„è¯­éŸ³..."):
            success, recognized_text = speech_recognizer.recognize_from_microphone()

        if success:
            st.success("è¯†åˆ«ç»“æœï¼š" + recognized_text)
            with st.spinner("AIæ­£åœ¨ç”Ÿæˆå›åº”..."):
                reply = llm_processor.process_text(recognized_text)
                st.info("AI å›ç­”ï¼š" + reply)
                st.audio(OUTPUT_WAV, format="audio/wav")

                text_to_speech.synthesize_speech(reply, OUTPUT_WAV)
                a2f_connector.push_audio_file(OUTPUT_WAV, A2F_INSTANCE)
        else:
            st.error("è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚")

elif mode == "è¯­éŸ³è¯„æµ‹":
    st.header("ğŸ—£ï¸ å‘éŸ³è¯„æµ‹")
    standard_text = st.text_input("è¯·è¾“å…¥ä½ è¦æœ—è¯»çš„æ ‡å‡†æ–‡æœ¬ï¼š")
    if st.button("ç‚¹å‡»å¼€å§‹æœ—è¯»å¹¶è¯„æµ‹") and standard_text:
        with st.spinner("è¯·æœ—è¯»æ–‡æœ¬ä¸­..."):
            success, recognized_text = speech_recognizer.recognize_and_save()

        if success:
            st.success("è¯†åˆ«ç»“æœï¼š" + recognized_text)
            with st.spinner("æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯„æµ‹..."):
                xml_result = speech_assessment.assess_speech(standard_text, USER_WAV)

            if xml_result:
                scores = speech_assessment.assessor.extract_scores(xml_result)
                feedback = llm_processor.process_assessment_result(standard_text, xml_result)

                st.subheader("ğŸ“Š è¯„æµ‹ç»“æœ")
                st.write(f"æ€»åˆ†: {scores.get('total_score', 0):.2f}")
                st.write(f"æµç•…åº¦: {scores.get('fluency_score', 0):.2f}")
                st.write(f"å®Œæ•´åº¦: {scores.get('integrity_score', 0):.2f}")
                st.write(f"å‘éŸ³åˆ†: {scores.get('phone_score', 0):.2f}")
                st.write(f"å£°è°ƒåˆ†: {scores.get('tone_score', 0):.2f}")

                st.subheader("ğŸ§¾ æ”¹è¿›å»ºè®®")
                st.info(feedback)

                text_to_speech.synthesize_speech(feedback, OUTPUT_WAV)
                st.audio(OUTPUT_WAV, format="audio/wav")
                a2f_connector.push_audio_file(OUTPUT_WAV, A2F_INSTANCE)
            else:
                st.error("è¯„æµ‹å¤±è´¥ã€‚è¯·ç¡®ä¿æ‚¨æœ—è¯»äº†æ­£ç¡®çš„å†…å®¹å¹¶é‡æ–°å°è¯•ã€‚")